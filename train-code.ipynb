{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YvN7lrePI2LK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils import data # 获取迭代数据\n",
    "from torch.autograd import Variable # 获取变量\n",
    "import torchvision\n",
    "from torchvision.datasets import mnist # 获取数据集\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler,WeightedRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import SequentialSampler\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "#print(pwd)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 直接指定项目根目录\n",
    "project_root = \"C:/Users/Admin/Mouse\"  # 替换成你的项目根目录路径\n",
    "datautils_path = os.path.join(project_root, 'datautils')\n",
    "model_path = os.path.join(project_root, 'model')\n",
    "\n",
    "# 添加路径\n",
    "sys.path.extend([datautils_path, model_path])\n",
    "\n",
    "# 导入模块\n",
    "from data_utils import load_mouse_data, process_mouse_data\n",
    "\n",
    "# 然后再导入其他模块\n",
    "from resnet_mouse import resnet50_1d\n",
    "from mouse_traj_classification import MouseNeuralNetwork, MouseNeuralNetwork2\n",
    "from new_optim import SWATS\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed = 3407\n",
    "# set the random seed for pytorch\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set the random seed for \"random\"\n",
    "random.seed(random_seed)\n",
    "\n",
    "# set the random seed for numpy\n",
    "np.random.seed(random_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 设置用户ID和窗口大小\n",
    "user_id = 23\n",
    "window_size = 400\n",
    "\n",
    "\n",
    "# 加载和处理数据\n",
    "X, label = process_mouse_data(\n",
    "    f'D:/论文数据/mouse/data/processed_data_user{user_id}/positive_samples_user{user_id}_{window_size}.json',\n",
    "    f'D:/论文数据/mouse/data/processed_data_user{user_id}/negative_samples_user{user_id}_{window_size}.json'\n",
    ")\n",
    "# 计算数据集划分长度\n",
    "\n",
    "train_len = math.floor(len(X)*0.7)\n",
    "test_len = len(X) - train_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总大小: 2627\n",
      "训练集大小: 1838\n",
      "测试集大小: 789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 检查数据集大小和batch_size的设置\n",
    "print(\"数据集总大小:\", len(X))\n",
    "print(\"训练集大小:\", train_len)\n",
    "print(\"测试集大小:\", test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iOxbaLA2JqEB",
    "outputId": "deebd76c-d2d6-4231-f497-299e3af59e17",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original distribution:\n",
      "Positive samples: 2389\n",
      "Negative samples: 238\n",
      "\n",
      "Distribution after split:\n",
      "Training set:\n",
      "Positive: 1672, Negative: 166\n",
      "Test set:\n",
      "Positive: 717, Negative: 72\n"
     ]
    }
   ],
   "source": [
    "# 创建整体数据集\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    X.float(),  # 确保输入数据是float类型\n",
    "    label.long() # 确保标签是long类型\n",
    ")\n",
    "\n",
    "# 统计正负样本数量\n",
    "pos_indices = (label == 0)  # 正样本索引\n",
    "neg_indices = (label == 1)  # 负样本索引\n",
    "pos_data = X[pos_indices]\n",
    "neg_data = X[neg_indices]\n",
    "pos_labels = label[pos_indices]\n",
    "neg_labels = label[neg_indices]\n",
    "\n",
    "print(f\"\\nOriginal distribution:\")\n",
    "print(f\"Positive samples: {len(pos_data)}\")\n",
    "print(f\"Negative samples: {len(neg_data)}\")\n",
    "\n",
    "# 计算正负样本中训练集和测试集的数量（70%训练，30%测试）\n",
    "n_pos = len(pos_data)\n",
    "n_neg = len(neg_data)\n",
    "\n",
    "train_pos_len = math.floor(n_pos * 0.7)\n",
    "test_pos_len = n_pos - train_pos_len\n",
    "\n",
    "train_neg_len = math.floor(n_neg * 0.7)\n",
    "test_neg_len = n_neg - train_neg_len\n",
    "\n",
    "# 随机打乱正负样本索引\n",
    "pos_shuffle = torch.randperm(n_pos)\n",
    "neg_shuffle = torch.randperm(n_neg)\n",
    "\n",
    "# 划分正样本：训练和测试\n",
    "train_pos = pos_data[pos_shuffle[:train_pos_len]]\n",
    "test_pos = pos_data[pos_shuffle[train_pos_len:]]\n",
    "\n",
    "train_pos_labels = pos_labels[pos_shuffle[:train_pos_len]]\n",
    "test_pos_labels = pos_labels[pos_shuffle[train_pos_len:]]\n",
    "\n",
    "# 划分负样本：训练和测试\n",
    "train_neg = neg_data[neg_shuffle[:train_neg_len]]\n",
    "test_neg = neg_data[neg_shuffle[train_neg_len:]]\n",
    "\n",
    "train_neg_labels = neg_labels[neg_shuffle[:train_neg_len]]\n",
    "test_neg_labels = neg_labels[neg_shuffle[train_neg_len:]]\n",
    "\n",
    "# 合并正负样本到训练集和测试集\n",
    "train_data = torch.cat([train_pos, train_neg])\n",
    "test_data = torch.cat([test_pos, test_neg])\n",
    "\n",
    "train_labels = torch.cat([train_pos_labels, train_neg_labels])\n",
    "test_labels = torch.cat([test_pos_labels, test_neg_labels])\n",
    "\n",
    "# 随机打乱训练集和测试集数据\n",
    "train_indices = torch.randperm(len(train_data))\n",
    "test_indices = torch.randperm(len(test_data))\n",
    "\n",
    "train_data = train_data[train_indices]\n",
    "train_labels = train_labels[train_indices]\n",
    "\n",
    "test_data = test_data[test_indices]\n",
    "test_labels = test_labels[test_indices]\n",
    "\n",
    "# 创建训练和测试数据集\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "# print(len(test_dataset))\n",
    "# 打印划分后的分布信息\n",
    "print(\"\\nDistribution after split:\")\n",
    "print(\"Training set:\")\n",
    "print(f\"Positive: {(train_labels == 0).sum().item()}, Negative: {(train_labels == 1).sum().item()}\")\n",
    "print(\"Test set:\")\n",
    "print(f\"Positive: {(test_labels == 0).sum().item()}, Negative: {(test_labels == 1).sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mfwYklq6KK_i",
    "outputId": "19c08381-49f4-4529-fa40-bb8676f0a61a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算类别权重\n",
    "class_count = [len(label[label == t]) for t in torch.unique(label)]\n",
    "weights = [2.0 if t == 1 else 1.0 for t in label.cpu().numpy()]  # 转换为numpy数组\n",
    "sample_weights = torch.FloatTensor(weights)\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "\n",
    "# 设置合适的batch_size\n",
    "train_batch_size = 100  # 较小的batch_size，可以更好地训练  \n",
    "test_batch_size = len(test_dataset)  # 测试集也可以用大一点的batch_size\n",
    "\n",
    "# 创建数据加载器\n",
    "X_train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "X_test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2627, 11, 400])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: D:/论文数据/mouse/data_pkl/processed_data_user23\n",
      "Train loader saved to: D:/论文数据/mouse/data_pkl/processed_data_user23/train_loader_user23_400.pkl\n",
      "Test loader saved to: D:/论文数据/mouse/data_pkl/processed_data_user23/test_loader_user23_400.pkl\n"
     ]
    }
   ],
   "source": [
    "# 保存各个数据加载器\n",
    "# 创建目录\n",
    "import pickle\n",
    "base_path = f'D:/论文数据/mouse/data_pkl/processed_data_user{user_id}'\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    print(f\"Created directory: {base_path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {base_path}\")\n",
    "train_path = f'{base_path}/train_loader_user{user_id}_{window_size}.pkl'\n",
    "test_path = f'{base_path}/test_loader_user{user_id}_{window_size}.pkl'\n",
    "\n",
    "with open(train_path, 'wb') as f:\n",
    "    pickle.dump(X_train_loader, f)\n",
    "with open(test_path, 'wb') as f:\n",
    "    pickle.dump(X_test_loader, f)\n",
    "\n",
    "print(f\"Train loader saved to: {train_path}\")\n",
    "print(f\"Test loader saved to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train loader from: D:/论文数据/mouse/data_pkl/processed_data_user23/train_loader_user23_400.pkl\n",
      "Loaded test loader from: D:/论文数据/mouse/data_pkl/processed_data_user23/test_loader_user23_400.pkl\n"
     ]
    }
   ],
   "source": [
    "# 读取数据加载器的代码：\n",
    "with open(train_path, 'rb') as f:\n",
    "    X_train_loader = pickle.load(f)\n",
    "with open(test_path, 'rb') as f:\n",
    "    X_test_loader = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded train loader from: {train_path}\")\n",
    "print(f\"Loaded test loader from: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V7RKo17yLTGS",
    "outputId": "c9a5bee3-d05c-48a4-fed3-6b4c334cc365",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据维度: torch.Size([100, 11, 400])\n",
      "标签维度: torch.Size([100])\n",
      "数据类型: torch.float32\n",
      "标签类型: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 首先从数据中获取序列长度和特征数量\n",
    "sequence_length = X.shape[2]  # 序列长度\n",
    "num_features = X.shape[1]     # 特征数量\n",
    "\n",
    "# 修改模型初始化部分\n",
    "model = MouseNeuralNetwork2(length_single_mouse_traj=50)\n",
    "model = model.to(device)  # 将模型移动到指定设备\n",
    "\n",
    "max_lr = 1e-3\n",
    "min_lr = 1e-7\n",
    "warmup_epochs = 10\n",
    "total_epochs = 300\n",
    "\n",
    "# 3. 初始化优化器 (在模型移动到GPU之后初始化优化器)\n",
    "optimizer = SWATS(model.parameters(), lr=1e-3)\n",
    "optim_SGD = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=1e-4, \n",
    "    momentum=0.80, \n",
    "    dampening=0, \n",
    "    weight_decay=1e-3, \n",
    "    nesterov=False\n",
    ")\n",
    "\n",
    "optim_ADAM = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=min_lr\n",
    ")\n",
    "\n",
    "# After warmup, use cosine annealing for (total_epochs - warmup_epochs) steps\n",
    "cosine_scheduler = CosineAnnealingLR(optim_ADAM, T_max=(total_epochs - warmup_epochs), eta_min=min_lr)\n",
    "\n",
    "# The multiplier is how much you multiply the initial LR to get the target LR at the end of warmup.\n",
    "# If initial LR = 1e-7 and we want 1e-3 after warmup:\n",
    "# multiplier = (desired_lr_after_warmup) / (initial_lr) = 1e-3 / 1e-7 = 10,000\n",
    "multiplier = max_lr / min_lr\n",
    "\n",
    "warmup_scheduler = GradualWarmupScheduler(optim_ADAM, multiplier=multiplier, total_epoch=warmup_epochs, after_scheduler=cosine_scheduler)\n",
    "\n",
    "# 检查第一个batch的数据\n",
    "for batch_data, batch_labels in X_train_loader:\n",
    "    print(\"数据维度:\", batch_data.shape)\n",
    "    print(\"标签维度:\", batch_labels.shape)\n",
    "    print(\"数据类型:\", batch_data.dtype)\n",
    "    print(\"标签类型:\", batch_labels.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt  # 导入matplotlib\n",
    "\n",
    "def train_ADAM(model, X_train_loader, optimizer=None, epoch=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    \n",
    "    losslist = []\n",
    "    correctlist = []\n",
    "    train_correctlist = []\n",
    "    \n",
    "    writer = SummaryWriter('/root/tf-logs')\n",
    "    \n",
    "    # 使用tqdm创建总进度条\n",
    "    pbar = tqdm(total=epoch, desc='Training Progress')\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        totalloss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for data in X_train_loader:\n",
    "            X, label = data\n",
    "            X = X.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X).squeeze()\n",
    "            loss = loss_function(pred, label)\n",
    "           \n",
    "            totalloss += loss.item()\n",
    "            num_batches += 1\n",
    "            _, pred_id = torch.max(pred, dim=1)\n",
    "            correct += (pred_id == label).sum().cpu().detach().numpy()\n",
    "            total += label.size(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Step the scheduler at the end of each epoch\n",
    "        warmup_scheduler.step()\n",
    "\n",
    "        # You can print or log the current LR to verify\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: LR = {current_lr}\")\n",
    "\n",
    "        train_acc = correct / total\n",
    "        avg_loss = totalloss / num_batches\n",
    "        losslist.append(avg_loss)\n",
    "        \n",
    "        # 更新进度条\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'acc': f'{train_acc:.4f}'\n",
    "        })\n",
    "        \n",
    "        # 每50个epoch记录一次\n",
    "        if (i + 1) % 100 == 0:\n",
    "            # clear_output(wait=True)  # 清除之前的输出\n",
    "            print(f'Epoch {i+1}/{epoch} - Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "            correctlist.append(train_acc)\n",
    "            train_correctlist.append(train_acc)\n",
    "            writer.add_scalar('Training loss', avg_loss, i)\n",
    "            writer.add_scalar('Training accuracy', train_acc, i)\n",
    "    \n",
    "    pbar.close()\n",
    "    writer.close()\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), losslist, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # 显示图表\n",
    "\n",
    "    # 可选：绘制准确率曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), correctlist, label='Training Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # 显示图表\n",
    "    \n",
    "    return optimizer, epoch, losslist, correctlist, train_correctlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MouseNeuralNetwork2(length_single_mouse_traj=window_size)\n",
    "optimizer = SWATS(model.parameters(), lr=1e-3)\n",
    "optim_ADAM = torch.optim.Adam(model.parameters(), lr=1e-6, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▉                                   | 50/2000 [00:28<16:32,  1.96it/s, loss=0.4109, acc=0.9211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/2000 - Loss: 0.4109, Accuracy: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|█▊                                 | 100/2000 [00:55<16:41,  1.90it/s, loss=0.3869, acc=0.9369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000 - Loss: 0.3869, Accuracy: 0.9369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|██▋                                | 150/2000 [01:22<16:23,  1.88it/s, loss=0.3682, acc=0.9483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/2000 - Loss: 0.3682, Accuracy: 0.9483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|███▌                               | 200/2000 [01:50<16:25,  1.83it/s, loss=0.3522, acc=0.9652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/2000 - Loss: 0.3522, Accuracy: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|████▍                              | 250/2000 [02:17<15:16,  1.91it/s, loss=0.3491, acc=0.9679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/2000 - Loss: 0.3491, Accuracy: 0.9679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█████▎                             | 300/2000 [02:44<14:55,  1.90it/s, loss=0.3449, acc=0.9701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/2000 - Loss: 0.3449, Accuracy: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|██████▏                            | 350/2000 [03:11<14:37,  1.88it/s, loss=0.3395, acc=0.9782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/2000 - Loss: 0.3395, Accuracy: 0.9782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|███████                            | 400/2000 [03:45<19:16,  1.38it/s, loss=0.3317, acc=0.9820]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/2000 - Loss: 0.3317, Accuracy: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|███████▉                           | 450/2000 [04:20<18:15,  1.42it/s, loss=0.3308, acc=0.9837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/2000 - Loss: 0.3308, Accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|████████▌                          | 491/2000 [04:50<18:20,  1.37it/s, loss=0.3275, acc=0.9869]"
     ]
    }
   ],
   "source": [
    "# 仅使用训练集进行训练\n",
    "optim_ADAM, epoch, losslist, correctlist, train_correctlist = train_ADAM(\n",
    "    model, \n",
    "    X_train_loader, \n",
    "    optimizer=optim_ADAM, \n",
    "    epoch=total_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "state = {'model': model.state_dict()}\n",
    "torch.save(state, f'D:/论文数据/mouse/model_pt/resnet/only-adam-user{user_id}_{window_size}-path.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = MouseNeuralNetwork2(X.shape[2])  # 替换为你的模型\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "==============  ==============  ==============  ==============  ==============  ==============  ==============  ============== "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                                       下面开始迁移学习"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==============  ==============  ==============  ==============  ==============  ==============  ==============  ============== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "model_transfer = MouseNeuralNetwork(window_size)  # 创建新模型\n",
    "model_pretrain = MouseNeuralNetwork2(window_size)  # 创建预训练模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练权重 - 使用weights_only=True提高安全性\n",
    "pretrain_path = f'D:/论文数据/mouse/model_pt/resnet/only-adam-user{user_id}_{window_size}-path.pt'\n",
    "checkpoint = torch.load(pretrain_path, weights_only=True)  # 添加weights_only=True\n",
    "print(f\"Loading pretrained model from: {pretrain_path}\")\n",
    "model_pretrain.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 ResNet1D 权重到 model_transfer.resnet\n",
    "model_transfer.resnet.load_state_dict(checkpoint['model'], strict=False)\n",
    "print(\"Pretrained ResNet1D weights loaded successfully.\")\n",
    "\n",
    "# 如果需要，可以选择冻结 ResNet1D 的参数\n",
    "for param in model_transfer.resnet.parameters():\n",
    "    param.requires_grad = True  # 冻结 ResNet1D 的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存的数据加载器\n",
    "import pickle\n",
    "base_path = f'D:/论文数据/mouse/data_pkl/processed_data_user{user_id}'\n",
    "\n",
    "train_path = f'{base_path}/train_loader_user{user_id}_{window_size}.pkl'\n",
    "test_path = f'{base_path}/test_loader_user{user_id}_{window_size}.pkl'\n",
    "print(f\"Loading data loaders for user {user_id}:\")\n",
    "with open(train_path, 'rb') as f:\n",
    "    X_train_loader = pickle.load(f)\n",
    "with open(test_path, 'rb') as f:\n",
    "    X_test_loader = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型移到GPU\n",
    "model_transfer = model_transfer.cuda()\n",
    "print(f\"Model moved to: {next(model_transfer.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr_transfer = 1e-3\n",
    "min_lr_transfer = 1e-7\n",
    "warmup_epochs_transfer = 10\n",
    "total_epochs_transfer = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化优化器\n",
    "optimizer = SWATS(model_transfer.parameters(), lr=1e-3)\n",
    "optim_SGD = torch.optim.SGD(model_transfer.parameters(), lr=1e-4, momentum=0.8, dampening=0, weight_decay=1e-3, nesterov=False)\n",
    "optim_ADAM_transfer = torch.optim.Adam(model_transfer.parameters(), lr=min_lr_transfer)\n",
    "\n",
    "# After warmup, use cosine annealing for (total_epochs - warmup_epochs) steps\n",
    "cosine_scheduler_transfer = CosineAnnealingLR(optim_ADAM_transfer, T_max=(total_epochs_transfer - warmup_epochs_transfer), eta_min=min_lr_transfer)\n",
    "\n",
    "# The multiplier is how much you multiply the initial LR to get the target LR at the end of warmup.\n",
    "# If initial LR = 1e-7 and we want 1e-3 after warmup:\n",
    "# multiplier = (desired_lr_after_warmup) / (initial_lr) = 1e-3 / 1e-7 = 10,000\n",
    "multiplier_transfer = max_lr_transfer / min_lr_transfer\n",
    "\n",
    "warmup_scheduler_transfer = GradualWarmupScheduler(optim_ADAM_transfer, multiplier=multiplier_transfer, total_epoch=warmup_epochs_transfer, after_scheduler=cosine_scheduler_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ADAM_transfer(model, X_train_loader, optimizer=None, epoch=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    \n",
    "    losslist = []\n",
    "    correctlist = []\n",
    "    train_correctlist = []\n",
    "    \n",
    "    writer = SummaryWriter('/root/tf-logs')\n",
    "    \n",
    "    # 使用tqdm创建总进度条\n",
    "    pbar = tqdm(total=epoch, desc='Training Progress')\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        totalloss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for data in X_train_loader:\n",
    "            X, label = data\n",
    "            X = X.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X).squeeze()\n",
    "            loss = loss_function(pred, label)\n",
    "           \n",
    "            totalloss += loss.item()\n",
    "            num_batches += 1\n",
    "            _, pred_id = torch.max(pred, dim=1)\n",
    "            correct += (pred_id == label).sum().cpu().detach().numpy()\n",
    "            total += label.size(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Step the scheduler at the end of each epoch\n",
    "        warmup_scheduler_transfer.step()\n",
    "\n",
    "        # You can print or log the current LR to verify\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: LR = {current_lr}\")\n",
    "\n",
    "        train_acc = correct / total\n",
    "        avg_loss = totalloss / num_batches\n",
    "        losslist.append(avg_loss)\n",
    "        \n",
    "        # 更新进度条\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'acc': f'{train_acc:.4f}'\n",
    "        })\n",
    "        \n",
    "        # 每50个epoch记录一次\n",
    "        if (i + 1) % 100 == 0:\n",
    "            # clear_output(wait=True)  # 清除之前的输出\n",
    "            print(f'Epoch {i+1}/{epoch} - Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "            correctlist.append(train_acc)\n",
    "            train_correctlist.append(train_acc)\n",
    "            writer.add_scalar('Training loss', avg_loss, i)\n",
    "            writer.add_scalar('Training accuracy', train_acc, i)\n",
    "    \n",
    "    pbar.close()\n",
    "    writer.close()\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), losslist, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # 显示图表\n",
    "\n",
    "    # 可选：绘制准确率曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epoch + 1), correctlist, label='Training Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # 显示图表\n",
    "    \n",
    "    return optimizer, epoch, losslist, correctlist, train_correctlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练迁移模型\n",
    "optim_ADAM, epoch, losslist, correctlist, train_correctlist = train_ADAM_transfer(\n",
    "    model_transfer, \n",
    "    X_train_loader, \n",
    "    optimizer=optim_ADAM, \n",
    "    epoch=total_epochs_transfer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存迁移后的模型\n",
    "transfer_save_path = f'D:/论文数据/mouse/model_pt/ending_model/gru-transfer-user{user_id}_{window_size}-path.pt'  # 修改为实际路径\n",
    "state = {\n",
    "    'model': model_transfer.state_dict(),\n",
    "    'epoch': epoch\n",
    "}\n",
    "torch.save(state, transfer_save_path)\n",
    "print(f\"Transfer model saved to: {transfer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = MouseNeuralNetwork(X.shape[2])  # 替换为你的模型\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==============  ==============  ==============  ==============  ==============  ==============  ==============  ============== "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                                       下面开始测试代码"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==============  ==============  ==============  ==============  ==============  ==============  ==============  ============== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import dill\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    WeightedRandomSampler,\n",
    "    SequentialSampler,\n",
    "    random_split,\n",
    "    TensorDataset\n",
    ")\n",
    "from torch.optim import Optimizer, SGD, Adam\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import mnist\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 3407\n",
    "torch.manual_seed(random_seed)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define project root and add paths\n",
    "project_root = \"C:/Users/Admin/Mouse\"  # Replace with your project root path\n",
    "datautils_path = os.path.join(project_root, 'datautils')\n",
    "model_path = os.path.join(project_root, 'model')\n",
    "sys.path.extend([datautils_path, model_path])\n",
    "\n",
    "# Import custom modules\n",
    "from data_utils import (\n",
    "    load_mouse_data,\n",
    "    process_mouse_data,\n",
    "    read_test_data_shape,\n",
    "    insert_new_test_data\n",
    ")\n",
    "from testdata_utils import process_predict_data\n",
    "from resnet_mouse import resnet50_1d\n",
    "from mouse_traj_classification import MouseNeuralNetwork, MouseNeuralNetwork2\n",
    "from new_optim import SWATS\n",
    "\n",
    "# Optional variables\n",
    "# user_id = 23\n",
    "# window_size = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(user_id, window_size, metrics_dict, csv_path):\n",
    "    \"\"\"\n",
    "    保存结果到CSV文件\n",
    "    Args:\n",
    "        user_id: 用户ID\n",
    "        window_size: 窗口大小\n",
    "        metrics_dict: 包含各项指标的字典\n",
    "        csv_path: CSV文件路径\n",
    "    \"\"\"\n",
    "    # 定义列名\n",
    "    columns = ['user_id', 'window_size', 'recall', 'accuracy', 'precision', 'F1', 'AUC', 'EER']\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # 创建新行数据\n",
    "    new_row = pd.DataFrame([{\n",
    "        'user_id': f'user{user_id}',\n",
    "        'window_size': window_size,\n",
    "        'recall': metrics_dict['recall'],\n",
    "        'accuracy': metrics_dict['accuracy'],\n",
    "        'precision': metrics_dict['precision'],\n",
    "        'F1': metrics_dict['f1'],\n",
    "        'AUC': metrics_dict['auc'],\n",
    "        'EER': metrics_dict['eer']\n",
    "    }])\n",
    "    \n",
    "    # 使用concat替代append\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # 保存到CSV\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def val(model, data_loader):\n",
    "    correct = 0 \n",
    "    fenmu = 0\n",
    "    New_label = []\n",
    "    \n",
    "    total_inference_time = 0\n",
    "    num_inferences = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        X, label = data\n",
    "        model.zero_grad()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        pred = model(X).squeeze()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_inference_time += (end_time - start_time)\n",
    "        num_inferences += X.size(0)\n",
    "        \n",
    "        _, pred_id = torch.max(pred, dim=1)\n",
    "        correct += (pred_id == label).sum().cpu().detach().numpy()\n",
    "        New_label.append(label)\n",
    "        fenmu += label.size(0)\n",
    "        \n",
    "    correct = correct / fenmu\n",
    "    avg_inference_time = total_inference_time / num_inferences\n",
    "    print(f\"Average inference time: {avg_inference_time:.6f} seconds\")\n",
    "    \n",
    "    return pred_id, pred, label\n",
    "\n",
    "def recall(predictions, labels):\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == 1 and labels[i] == 1:\n",
    "            TP += 1\n",
    "        elif predictions[i] == 1 and labels[i] == 0:\n",
    "            FP += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 0:\n",
    "            TN += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 1:\n",
    "            FN += 1\n",
    "    if (TP + FN) == 0:\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "def et_TPFN(predictions, labels):\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == 1 and labels[i] == 1:\n",
    "            TP += 1\n",
    "        elif predictions[i] == 1 and labels[i] == 0:\n",
    "            FP += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 0:\n",
    "            TN += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 1:\n",
    "            FN += 1\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def pre(predictions, labels):\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == 1 and labels[i] == 1:\n",
    "            TP += 1\n",
    "        elif predictions[i] == 1 and labels[i] == 0:\n",
    "            FP += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 0:\n",
    "            TN += 1\n",
    "        elif predictions[i] == 0 and labels[i] == 1:\n",
    "            FN += 1\n",
    "    return TP/(FP+TP)\n",
    "# 在评估函数部分添加新的指标计算函数\n",
    "def calculate_f1(precision, recall):\n",
    "    \"\"\"计算F1分数\"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def calculate_eer(fpr, tpr):\n",
    "    \"\"\"计算EER (Equal Error Rate)\"\"\"\n",
    "    fnr = 1 - tpr\n",
    "    # EER是当FAR(FPR)等于FRR(FNR)时的值\n",
    "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. 加载原始测试数据\n",
    "test_path = f'D:/论文数据/mouse/data_pkl/processed_data_user{user_id}/test_loader_user{user_id}_{window_size}.pkl'\n",
    "print(f\"\\nLoading test data from: {test_path}\")\n",
    "with open(test_path, 'rb') as f:\n",
    "    X_test_loader = dill.load(f)\n",
    "\n",
    "# 将所有测试样本整合为单独的tensor（X_all, y_all）\n",
    "X_list = []\n",
    "y_list = []\n",
    "for data in X_test_loader:\n",
    "    X_batch, y_batch = data\n",
    "    X_list.append(X_batch)\n",
    "    y_list.append(y_batch)\n",
    "\n",
    "X_all = torch.cat(X_list, dim=0)\n",
    "y_all = torch.cat(y_list, dim=0)\n",
    "\n",
    "print(f\"\\nOriginal Test Dataset Statistics:\")\n",
    "print(f\"Total number of samples: {len(X_all)}\")\n",
    "print(f\"Positive (0): {(y_all == 0).sum().item()}\")\n",
    "print(f\"Negative (1): {(y_all == 1).sum().item()}\")\n",
    "print(f\"Batch size: {X_test_loader.batch_size}\")\n",
    "print(f\"Number of batches: {len(X_test_loader)}\")\n",
    "print(f\"Sample shape: {X_all.shape[1:]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 加载和处理预测数据\n",
    "predict_path = f'D:/论文数据/mouse/data/processed_data_user{user_id}/predict_samples_user{user_id}_{window_size}.json'\n",
    "print(f\"\\nLoading predict data from: {predict_path}\")\n",
    "\n",
    "with open(predict_path, 'r') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "# 将预测数据转换为tensor\n",
    "trajectories = []\n",
    "feature_names = predict_data['metadata']['feature_names']\n",
    "for sample in predict_data['samples']:\n",
    "    trajectory = np.array([[step[feature] for feature in feature_names] for step in sample])\n",
    "    trajectories.append(trajectory)\n",
    "\n",
    "X_predict = np.array(trajectories)\n",
    "X_predict = torch.FloatTensor(X_predict).transpose(1, 2)  # [N, features, time]\n",
    "\n",
    "# 原始测试数据是否有4维？如果有，需要与预测数据统一\n",
    "# 检查原始测试数据的维度，若是4维，这里假设需要在feature维前再增加一维\n",
    "if len(X_all.shape) == 4 and X_predict.dim() == 3:\n",
    "    X_predict = X_predict.unsqueeze(dim=1)  # 在 feature 前面增加一个维度，使 shape 一致\n",
    "elif len(X_all.shape) == 3 and X_predict.dim() == 4:\n",
    "    # 若原始数据是3维，而预测是4维，则需要去掉一维\n",
    "    X_predict = X_predict.squeeze(dim=1)\n",
    "\n",
    "print(f\"Predict samples shape after processing: {X_predict.shape}\")\n",
    "\n",
    "# 3. 固定选择195个正样本、19个负样本和19个预测样本\n",
    "desired_pos = 456\n",
    "desired_neg = 44\n",
    "desired_predict = 44\n",
    "\n",
    "pos_indices = torch.where(y_all == 0)[0]\n",
    "neg_indices = torch.where(y_all == 1)[0]\n",
    "\n",
    "# 随机选择所需数量的正负样本\n",
    "if len(pos_indices) < desired_pos:\n",
    "    raise ValueError(f\"Not enough positive samples. Required {desired_pos}, got {len(pos_indices)}.\")\n",
    "if len(neg_indices) < desired_neg:\n",
    "    raise ValueError(f\"Not enough negative samples. Required {desired_neg}, got {len(neg_indices)}.\")\n",
    "\n",
    "pos_indices = pos_indices[torch.randperm(len(pos_indices))[:desired_pos]]\n",
    "neg_indices = neg_indices[torch.randperm(len(neg_indices))[:desired_neg]]\n",
    "\n",
    "X_selected = torch.cat([X_all[pos_indices], X_all[neg_indices]], dim=0)\n",
    "y_selected = torch.cat([y_all[pos_indices], y_all[neg_indices]], dim=0)\n",
    "\n",
    "# 从预测数据中随机选择19个样本\n",
    "if len(X_predict) < desired_predict:\n",
    "    print(f\"Warning: Only {len(X_predict)} predict samples available, less than desired {desired_predict}\")\n",
    "    desired_predict = len(X_predict)  # 若不够则使用全部\n",
    "indices = torch.randperm(len(X_predict))[:desired_predict]\n",
    "X_insert = X_predict[indices]\n",
    "\n",
    "# 创建对应的预测样本标签（使用1表示负样本）\n",
    "label_insert = torch.ones(len(X_insert), dtype=torch.int64)\n",
    "\n",
    "# 合并预测样本与原始样本\n",
    "X_final = torch.cat([X_selected, X_insert], dim=0)\n",
    "y_final = torch.cat([y_selected, label_insert], dim=0)\n",
    "\n",
    "print(\"\\nFinal Test Dataset Statistics:\")\n",
    "print(f\"Total number of samples: {len(X_final)}\")\n",
    "print(f\"Positive (0): {(y_final == 0).sum().item()}\")\n",
    "print(f\"Negative (1): {(y_final == 1).sum().item()}\")\n",
    "print(f\"Data shape: {X_final.shape[1:]}\")\n",
    "\n",
    "# 如有需要，可将X_final, y_final打乱\n",
    "final_indices = torch.randperm(len(X_final))\n",
    "X_final = X_final[final_indices]\n",
    "y_final = y_final[final_indices]\n",
    "\n",
    "# 这样X_final和y_final就是最终的测试数据集（195正,19负,19预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"Creating new test DataLoader from the selected final samples...\")\n",
    "\n",
    "# 根据X_final和y_final创建数据集与DataLoader\n",
    "test_dataset = torch.utils.data.TensorDataset(X_final, y_final)\n",
    "# 根据需要设置batch_size和是否shuffle\n",
    "new_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# 打印插入后的shape信息（单个样本的形状）\n",
    "shape_new_single_mouse_traj = X_final.shape[1:]\n",
    "print(f\"Shape after insertion: {shape_new_single_mouse_traj}\")\n",
    "\n",
    "# 检查new_test_dataloader的详细信息\n",
    "print(\"\\nNew Test DataLoader Details:\")\n",
    "total_samples = 0\n",
    "total_batches = 0\n",
    "batch_sizes = []\n",
    "\n",
    "for batch_idx, (X, labels) in enumerate(new_test_dataloader):\n",
    "    total_samples += len(X)\n",
    "    total_batches += 1\n",
    "    batch_sizes.append(len(X))\n",
    "    if batch_idx == 0:\n",
    "        print(f\"Single batch shape: {X.shape}\")\n",
    "        # 假设X的shape为 [batch, feature, seq_length] (或根据实际数据形状)\n",
    "        print(f\"Features dimension: {X.shape[1]}\")\n",
    "        print(f\"Sequence length: {X.shape[2]}\")\n",
    "\n",
    "print(f\"\\nTotal number of samples: {total_samples}\")\n",
    "print(f\"Number of batches: {total_batches}\")\n",
    "print(f\"Batch sizes: {batch_sizes}\")\n",
    "\n",
    "# 检查标签分布\n",
    "all_labels = []\n",
    "for _, labels in new_test_dataloader:\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 创建和加载模型\n",
    "New_model_new_test = MouseNeuralNetwork(shape_new_single_mouse_traj[-1])\n",
    "model_path = f'D:/论文数据/mouse/model_pt/ending_model/gru-transfer-user{user_id}_{window_size}-path.pt'\n",
    "print(f\"\\nLoading model from: {model_path}\")\n",
    "checkpoint = torch.load(model_path, weights_only=True) \n",
    "New_model_new_test.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 4. 评估原始测试集\n",
    "print(\"\\nEvaluating original test set...\")\n",
    "d_c = []      # recall\n",
    "d_acc = []    # accuracy\n",
    "d_precsion = [] # precision\n",
    "d_f1 = []     # F1 score\n",
    "d_auc = []    # AUC\n",
    "d_eer = []    # EER\n",
    "d_confusion_matrices = []  # 混淆矩阵\n",
    "\n",
    "for i in range(1):\n",
    "    pred_id_list, pred_list, label_list = val(New_model_new_test, X_test_loader)\n",
    "    \n",
    "    # 确保数据格式正确\n",
    "    if isinstance(pred_list, torch.Tensor):\n",
    "        pred_probs = pred_list.detach().cpu().numpy()\n",
    "    else:\n",
    "        pred_probs = pred_list\n",
    "        \n",
    "    if isinstance(label_list, torch.Tensor):\n",
    "        labels = label_list.cpu().numpy()\n",
    "    else:\n",
    "        labels = label_list\n",
    "    \n",
    "    # 计算基本指标\n",
    "    zhaohui = recall(pred_id_list, label_list)\n",
    "    c = et_TPFN(pred_id_list, label_list)\n",
    "    acc = (c[0]+c[2])/( c[0]+c[1]+c[2]+c[3])\n",
    "    precision = pre(pred_id_list, label_list)\n",
    "    \n",
    "    # 计算F1分数\n",
    "    f1 = calculate_f1(precision, zhaohui)\n",
    "    \n",
    "    # 计算ROC和AUC\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, pred_probs[:, 1])\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # 计算EER\n",
    "    eer = calculate_eer(fpr, tpr)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    conf_matrix = metrics.confusion_matrix(labels, pred_id_list)\n",
    "    \n",
    "    # 保存所有指标\n",
    "    d_c.append(zhaohui)\n",
    "    d_acc.append(acc)\n",
    "    d_precsion.append(precision)\n",
    "    d_f1.append(f1)\n",
    "    d_auc.append(auc)\n",
    "    d_eer.append(eer)\n",
    "    d_confusion_matrices.append(conf_matrix)\n",
    "    \n",
    "    print(f\"Round {i+1}/10 completed\")\n",
    "\n",
    "print(\"\\nOriginal Test Set Results:\")\n",
    "print(f\"Average Recall: {np.mean(d_c):.4f} ± {np.std(d_c):.4f}\")\n",
    "print(f\"Average Accuracy: {np.mean(d_acc):.4f} ± {np.std(d_acc):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(d_precsion):.4f} ± {np.std(d_precsion):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(d_f1):.4f} ± {np.std(d_f1):.4f}\")\n",
    "print(f\"Average AUC: {np.mean(d_auc):.4f} ± {np.std(d_auc):.4f}\")\n",
    "print(f\"Average EER: {np.mean(d_eer):.4f} ± {np.std(d_eer):.4f}\")\n",
    "\n",
    "# 打印混淆矩阵\n",
    "for i, conf_matrix in enumerate(d_confusion_matrices):\n",
    "    print(f\"\\nConfusion Matrix for Round {i+1}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "# 保存原始测试集结果\n",
    "original_metrics = {\n",
    "    'recall': np.mean(d_c),\n",
    "    'accuracy': np.mean(d_acc),\n",
    "    'precision': np.mean(d_precsion),\n",
    "    'f1': np.mean(d_f1),\n",
    "    'auc': np.mean(d_auc),\n",
    "    'eer': np.mean(d_eer)\n",
    "}\n",
    "\n",
    "# 定义基础路径\n",
    "base_path = f'D:/论文数据/mouse/RESULTS'\n",
    "\n",
    "# 确保目录存在\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# 构建文件名，添加 user_id 和 window_size\n",
    "filename = f'original_test_results_user{user_id}.csv'\n",
    "\n",
    "# 拼接完整的文件路径\n",
    "full_path = os.path.join(base_path, filename)\n",
    "\n",
    "# 调用保存函数\n",
    "save_results_to_csv(user_id, window_size, original_metrics, full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 评估插入预测样本后的测试集\n",
    "print(\"\\nEvaluating test set with predict samples...\")\n",
    "d_c_2 = []\n",
    "d_acc_2 = []\n",
    "d_precsion_2 = []\n",
    "d_f1_2 = []\n",
    "d_auc_2 = []    # 添加AUC列表\n",
    "d_eer_2 = []\n",
    "d_confusion_matrices2 = []  # 混淆矩阵\n",
    "for i in range(1):\n",
    "    pred_id_list, pred_list, label_list = val(New_model_new_test, new_test_dataloader)\n",
    "    # 确保数据格式正确\n",
    "    if isinstance(pred_list, torch.Tensor):\n",
    "        pred_probs = pred_list.detach().cpu().numpy()\n",
    "    else:\n",
    "        pred_probs = pred_list\n",
    "        \n",
    "    if isinstance(label_list, torch.Tensor):\n",
    "        labels = label_list.cpu().numpy()\n",
    "    else:\n",
    "        labels = label_list\n",
    "    \n",
    "    # 计算基本指标\n",
    "    zhaohui = recall(pred_id_list, label_list)\n",
    "    c = et_TPFN(pred_id_list, label_list)\n",
    "    acc = (c[0]+c[2])/( c[0]+c[1]+c[2]+c[3])\n",
    "    precision = pre(pred_id_list, label_list)\n",
    "    \n",
    "    # 计算F1分数\n",
    "    f1 = calculate_f1(precision, zhaohui)\n",
    "    \n",
    "    # 计算ROC和AUC\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, pred_probs[:, 1])\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # 计算EER\n",
    "    eer = calculate_eer(fpr, tpr)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    conf_matrix = metrics.confusion_matrix(labels, pred_id_list)\n",
    "    \n",
    "    # 保存所有指标\n",
    "    d_c_2.append(zhaohui)\n",
    "    d_acc_2.append(acc)\n",
    "    d_precsion_2.append(precision)\n",
    "    d_f1_2.append(f1)\n",
    "    d_auc_2.append(auc)    # 保存AUC值\n",
    "    d_eer_2.append(eer)\n",
    "    d_confusion_matrices2.append(conf_matrix)\n",
    "    print(f\"Round {i+1}/10 completed\")\n",
    "\n",
    "print(\"\\nTest Set with Predict Samples Results:\")\n",
    "print(f\"Average Recall: {np.mean(d_c_2):.4f} ± {np.std(d_c_2):.4f}\")\n",
    "print(f\"Average Accuracy: {np.mean(d_acc_2):.4f} ± {np.std(d_acc_2):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(d_precsion_2):.4f} ± {np.std(d_precsion_2):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(d_f1_2):.4f} ± {np.std(d_f1_2):.4f}\")\n",
    "print(f\"Average AUC: {np.mean(d_auc_2):.4f} ± {np.std(d_auc_2):.4f}\")    # 打印AUC结果\n",
    "print(f\"Average EER: {np.mean(d_eer_2):.4f} ± {np.std(d_eer_2):.4f}\")\n",
    "\n",
    "# 保存插入预测样本后的测试集结果\n",
    "merged_metrics = {\n",
    "    'recall': np.mean(d_c_2),\n",
    "    'accuracy': np.mean(d_acc_2),\n",
    "    'precision': np.mean(d_precsion_2),\n",
    "    'f1': np.mean(d_f1_2),\n",
    "    'auc': np.mean(d_auc_2),    # 添加AUC到保存结果中\n",
    "    'eer': np.mean(d_eer_2)\n",
    "}\n",
    "\n",
    "# 打印混淆矩阵\n",
    "for i, conf_matrix in enumerate(d_confusion_matrices2):\n",
    "    print(f\"\\nConfusion Matrix for Round {i+1}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "# 定义基础路径\n",
    "base_path = f'D:/论文数据/mouse/RESULTS'\n",
    "\n",
    "# 确保目录存在\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# 构建文件名，添加 user_id 和 window_size\n",
    "filename = f'merged_test_results_user{user_id}.csv'\n",
    "\n",
    "# 拼接完整的文件路径\n",
    "full_path = os.path.join(base_path, filename)\n",
    "\n",
    "\n",
    "\n",
    "# 调用保存函数\n",
    "save_results_to_csv(user_id, window_size, merged_metrics, full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 计算和绘制ROC曲线\n",
    "from sklearn import metrics\n",
    "\n",
    "# 计算第一个测试集的ROC\n",
    "pred_id, pred, label = val(New_model_new_test, X_test_loader)\n",
    "fpr1, tpr1, _ = metrics.roc_curve(label, pred[:,1].detach().numpy())\n",
    "auc1 = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "# 计算第二个测试集的ROC\n",
    "pred_id2, pred2, label2 = val(New_model_new_test, new_test_dataloader)\n",
    "fpr2, tpr2, _ = metrics.roc_curve(label2, pred2[:,1].detach().numpy())\n",
    "auc2 = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "# 绘制ROC曲线对比图\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr1, tpr1, 'b-', label=f'Original Test Set (AUC = {auc1:.4f})')\n",
    "plt.plot(fpr2, tpr2, 'r--', label=f'Test Set with Predict Samples (AUC = {auc2:.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curves Comparison - User {user_id}')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 7. 绘制性能指标对比图\n",
    "metrics = ['Recall', 'Accuracy', 'Precision']\n",
    "test1_scores = [np.mean(d_c), np.mean(d_acc), np.mean(d_precsion)]\n",
    "test2_scores = [np.mean(d_c_2), np.mean(d_acc_2), np.mean(d_precsion_2)]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x - width/2, test1_scores, width, label='Original Test Set')\n",
    "plt.bar(x + width/2, test2_scores, width, label='Test Set with Predict Samples')\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 更新性能指标对比图，包含AUC\n",
    "metrics = ['Recall', 'Accuracy', 'Precision', 'F1', 'AUC', 'EER']\n",
    "test1_scores = [np.mean(d_c), np.mean(d_acc), np.mean(d_precsion), \n",
    "                np.mean(d_f1), np.mean(d_auc), np.mean(d_eer)]\n",
    "test2_scores = [np.mean(d_c_2), np.mean(d_acc_2), np.mean(d_precsion_2), \n",
    "                np.mean(d_f1_2), np.mean(d_auc_2), np.mean(d_eer_2)]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, test1_scores, width, label='Original Test Set')\n",
    "plt.bar(x + width/2, test2_scores, width, label='Test Set with Predict Samples')\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
